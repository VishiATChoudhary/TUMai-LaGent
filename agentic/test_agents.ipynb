{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai.models.chat_completion import ChatMessage\n",
    "from mistralai.client import MistralClient\n",
    "\n",
    "#from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "MISTRAL_API_KEY = \"mgNvjI0hi3WcPdMvPMVgy8LKQFQhVtBx\"  # Replace with your Mistral API key\n",
    "\n",
    "\n",
    "client = MistralClient(api_key=MISTRAL_API_KEY)\n",
    "model_id = \"mistral-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mistralai.client.MistralClient at 0x221bbfdd1f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "MistralAPIException",
     "evalue": "Cannot stream response. Status: 400",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMistralAPIException\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m messages = [\n\u001b[32m      2\u001b[39m     ChatMessage(role=\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, content=\u001b[33m\"\u001b[39m\u001b[33mWhat is the capital of France?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m ]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m chat_response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmistral-caht\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(chat_response.choices[\u001b[32m0\u001b[39m].message.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\airke\\anaconda3\\envs\\lagent\\Lib\\site-packages\\mistralai\\client.py:160\u001b[39m, in \u001b[36mMistralClient.chat\u001b[39m\u001b[34m(self, model, messages, temperature, max_tokens, top_p, random_seed, safe_mode, safe_prompt)\u001b[39m\n\u001b[32m    147\u001b[39m request = \u001b[38;5;28mself\u001b[39m._make_chat_request(\n\u001b[32m    148\u001b[39m     model,\n\u001b[32m    149\u001b[39m     messages,\n\u001b[32m   (...)\u001b[39m\u001b[32m    155\u001b[39m     safe_prompt=safe_mode \u001b[38;5;129;01mor\u001b[39;00m safe_prompt,\n\u001b[32m    156\u001b[39m )\n\u001b[32m    158\u001b[39m single_response = \u001b[38;5;28mself\u001b[39m._request(\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, request, \u001b[33m\"\u001b[39m\u001b[33mv1/chat/completions\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msingle_response\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mChatCompletionResponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m MistralException(\u001b[33m\"\u001b[39m\u001b[33mNo response received\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\airke\\anaconda3\\envs\\lagent\\Lib\\site-packages\\mistralai\\client.py:93\u001b[39m, in \u001b[36mMistralClient._request\u001b[39m\u001b[34m(self, method, json, path, stream, attempt)\u001b[39m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     86\u001b[39m         response = \u001b[38;5;28mself\u001b[39m._client.request(\n\u001b[32m     87\u001b[39m             method,\n\u001b[32m     88\u001b[39m             url,\n\u001b[32m     89\u001b[39m             headers=headers,\n\u001b[32m     90\u001b[39m             json=json,\n\u001b[32m     91\u001b[39m         )\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MistralConnectionException(\u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\airke\\anaconda3\\envs\\lagent\\Lib\\site-packages\\mistralai\\client_base.py:92\u001b[39m, in \u001b[36mClientBase._check_response\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, response: Response) -> Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_response_status_codes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m     json_response: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = response.json()\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m json_response:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\airke\\anaconda3\\envs\\lagent\\Lib\\site-packages\\mistralai\\client_base.py:79\u001b[39m, in \u001b[36mClientBase._check_response_status_codes\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MistralAPIStatusException.from_response(\n\u001b[32m     75\u001b[39m         response,\n\u001b[32m     76\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot stream response. Status: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     77\u001b[39m     )\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m400\u001b[39m <= response.status_code < \u001b[32m500\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MistralAPIException.from_response(\n\u001b[32m     80\u001b[39m         response,\n\u001b[32m     81\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot stream response. Status: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     82\u001b[39m     )\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code >= \u001b[32m500\u001b[39m:\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MistralException(\n\u001b[32m     85\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected server error (status \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     86\u001b[39m     )\n",
      "\u001b[31mMistralAPIException\u001b[39m: Cannot stream response. Status: 400"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    ChatMessage(role=\"user\", content=\"What is the capital of France?\")\n",
    "]\n",
    "\n",
    "chat_response = client.chat(\n",
    "    model=\"mistral-small\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(chat_response.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.tools import tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "tavily_tool = TavilySearchResults(max_results=5)\n",
    "\n",
    "# Warning: This executes code locally, which can be unsafe when not sandboxed\n",
    "\n",
    "repl = PythonREPL()\n",
    "\n",
    "\n",
    "@tool\n",
    "def python_repl_tool(\n",
    "    code: Annotated[str, \"The python code to execute to generate your chart.\"],\n",
    "):\n",
    "    \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    result_str = f\"Successfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\"\n",
    "    return (\n",
    "        result_str + \"\\n\\nIf you have completed all tasks, respond with FINAL ANSWER.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = BedrockLLM(\n",
    "    client=client,\n",
    "    model_id=model_id,\n",
    "    model_kwargs={\"temperature\": 0.0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_system_prompt(suffix: str) -> str:\n",
    "    return (\n",
    "        \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "        \" Use the provided tools to progress towards answering the question.\"\n",
    "        \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "        \" will help where you left off. Execute what you can to make progress.\"\n",
    "        \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "        \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "        f\"\\n{suffix}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.graph import MessagesState, END\n",
    "from langgraph.typing_extensions.TypedDict import Command\n",
    "\n",
    "\n",
    "\n",
    "def get_next_node(last_message: BaseMessage, goto: str):\n",
    "    if \"FINAL ANSWER\" in last_message.content:\n",
    "        # Any agent decided the work is done\n",
    "        return END\n",
    "    return goto\n",
    "\n",
    "\n",
    "# Research agent and node\n",
    "research_agent = create_react_agent(\n",
    "    llm,\n",
    "    tools=[tavily_tool],\n",
    "    prompt=make_system_prompt(\n",
    "        \"You can only do research. You are working with a chart generator colleague.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "def research_node(\n",
    "    state: MessagesState,\n",
    ") -> Command[Literal[\"chart_generator\", END]]:\n",
    "    result = research_agent.invoke(state)\n",
    "    goto = get_next_node(result[\"messages\"][-1], \"chart_generator\")\n",
    "    # wrap in a human message, as not all providers allow\n",
    "    # AI message at the last position of the input messages list\n",
    "    result[\"messages\"][-1] = HumanMessage(\n",
    "        content=result[\"messages\"][-1].content, name=\"researcher\"\n",
    "    )\n",
    "    return Command(\n",
    "        update={\n",
    "            # share internal message history of research agent with other agents\n",
    "            \"messages\": result[\"messages\"],\n",
    "        },\n",
    "        goto=goto,\n",
    "    )\n",
    "\n",
    "\n",
    "# Chart generator agent and node\n",
    "# NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION, WHICH CAN BE UNSAFE WHEN NOT SANDBOXED\n",
    "chart_agent = create_react_agent(\n",
    "    llm,\n",
    "    [python_repl_tool],\n",
    "    prompt=make_system_prompt(\n",
    "        \"You can only generate charts. You are working with a researcher colleague.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "def chart_node(state: MessagesState) -> Command[Literal[\"researcher\", END]]:\n",
    "    result = chart_agent.invoke(state)\n",
    "    goto = get_next_node(result[\"messages\"][-1], \"researcher\")\n",
    "    # wrap in a human message, as not all providers allow\n",
    "    # AI message at the last position of the input messages list\n",
    "    result[\"messages\"][-1] = HumanMessage(\n",
    "        content=result[\"messages\"][-1].content, name=\"chart_generator\"\n",
    "    )\n",
    "    return Command(\n",
    "        update={\n",
    "            # share internal message history of chart agent with other agents\n",
    "            \"messages\": result[\"messages\"],\n",
    "        },\n",
    "        goto=goto,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
